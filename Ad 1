FROM apache/airflow:3.0.6-python3.9

# Switch to root to extract Spark binaries
USER root

# Copy Spark binaries into container
COPY spark-3.5.2-bin-hadoop3.tgz /opt/
RUN tar -xzf /opt/spark-3.5.2-bin-hadoop3.tgz -C /opt/ && \
    mv /opt/spark-3.5.2-bin-hadoop3 /opt/spark && \
    rm /opt/spark-3.5.2-bin-hadoop3.tgz

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH

# Switch back to airflow user
USER airflow

# Copy DAGs
COPY dags/ /opt/airflow/dags
